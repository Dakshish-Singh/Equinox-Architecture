{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"fiDX9dzIU8wv"},"outputs":[],"source":["!pip install datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Co8cfUW4VEb1"},"outputs":[],"source":["import jax\n","import jax.numpy as jnp\n","from jax import grad, jit, random\n","from jax import lax\n","from jax.tree_util import tree_map\n","import numpy as np\n","from jax import grad, jit, random\n","from jax import lax\n","from jax.tree_util import tree_map\n","from transformers import GPT2Model, GPT2Tokenizer\n","import torch\n","import numpy as np\n","from datasets import load_dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1vh1agGjVGCJ"},"outputs":[],"source":["# Load the GPT-2 tokenizer and model\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","model_gpt = GPT2Model.from_pretrained('gpt2')\n","vocab = tokenizer.get_vocab()\n","model_gpt.eval()\n","\n","def break_word_gpt(text):\n","    tokens = tokenizer.tokenize(text)\n","    return tokens\n","\n","def break_into_id(text):\n","    return tokenizer.encode(text, add_special_tokens=False)\n","\n","def encode_word_gpt(word):\n","    token_ids = tokenizer.encode(word, add_special_tokens=False)\n","    token_tensor = torch.tensor([token_ids])\n","    with torch.no_grad():\n","        token_embeddings = model_gpt.wte(token_tensor)\n","    return token_embeddings\n","\n","# Load the PTB text dataset\n","# dataset = load_dataset('ptb_text_only')\n","dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\")\n","train_data = dataset['train']\n","test_data = dataset['test']\n","print(dataset)\n","\n","def read_batch_file(start_index, size, data):\n","    data_token = []\n","    for sentence_pair_index in range(start_index, start_index + size):\n","        if sentence_pair_index < len(data):\n","          if len(data[sentence_pair_index][\"text\"]) > 0:\n","            data_token.append(break_word_gpt(data[sentence_pair_index][\"text\"]))\n","    return data_token\n","def make_data(no_token,data):\n","  data_tokens = read_batch_file(0, int(len(data)/1), data)\n","  encode_tokens , encode_tokens_test ,x , y, y_LLM = [],[],[],[],[]\n","  temp = np.zeros(2*768)\n","  for i in data_tokens:\n","      encode_tokens.append(encode_word_gpt(i)[0])\n","  for i in range(int(len(data_tokens)/1)):\n","      if len(data_tokens[i]) > no_token:\n","          y.append(encode_tokens[i][no_token])\n","          y_LLM.append(break_into_id([data_tokens[i][no_token]]))\n","  x = np.zeros(shape=(len(y),no_token // 2,2*768))\n","  count_index = 0\n","  for i in range(int(len(data_tokens)/1)):\n","      if len(data_tokens[i]) > no_token:\n","          for k in range(no_token):\n","              if k % 2 == 0:\n","                  x[count_index][int(k/2)][:768] = encode_tokens[i][k]\n","              else:\n","                  x[count_index][int(k/2)][768:] = encode_tokens[i][k]\n","          count_index += 1\n","  x = np.transpose(np.transpose(x))\n","  y = jnp.array(y)\n","  x = jnp.array(x)\n","  return (x, y, y_LLM)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XtKUAYECVHxw"},"outputs":[],"source":["x, y, LLM_y = make_data(64, test_data)\n","x_test, y_test, LLM_y_test = make_data(64, test_data)\n","print(x.shape, y.shape)\n","print(x_test.shape, y_test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4N3d4bbmlFeo"},"outputs":[],"source":["print(LLM_y[:-10])\n","print(LLM_y_test[:-10])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S8pFDt1AVIyq"},"outputs":[],"source":["# Initialize network parameters\n","def init_network_params(layer_sizes, key):\n","    keys = random.split(key, len(layer_sizes))\n","    return [(random.normal(k, (m, n)) * 0.01, random.normal(k, (n,)) * 0.01)\n","            for m, n, k in zip(layer_sizes[:-1], layer_sizes[1:], keys)]\n","\n","# Initialize Adam optimizer state\n","def init_adam_state(params):\n","    return [(jnp.zeros_like(w), jnp.zeros_like(w), jnp.zeros_like(b), jnp.zeros_like(b)) for w, b in params]\n","\n","# Adam optimizer update\n","def adam_update(params, grads, opt_state, t, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8):\n","    new_params = []\n","    new_opt_state = []\n","\n","    for (w, b), (dw, db), (m_w, v_w, m_b, v_b) in zip(params, grads, opt_state):\n","        m_w = beta1 * m_w + (1 - beta1) * dw\n","        v_w = beta2 * v_w + (1 - beta2) * (dw ** 2)\n","        m_b = beta1 * m_b + (1 - beta1) * db\n","        v_b = beta2 * v_b + (1 - beta2) * (db ** 2)\n","\n","        m_w_hat = m_w / (1 - beta1 ** t)\n","        v_w_hat = v_w / (1 - beta2 ** t)\n","        m_b_hat = m_b / (1 - beta1 ** t)\n","        v_b_hat = v_b / (1 - beta2 ** t)\n","\n","        w = w - lr * m_w_hat / (jnp.sqrt(v_w_hat) + eps)\n","        b = b - lr * m_b_hat / (jnp.sqrt(v_b_hat) + eps)\n","\n","        new_params.append((w, b))\n","        new_opt_state.append((m_w, v_w, m_b, v_b))\n","\n","    return new_params, new_opt_state\n","\n","# Activation functions\n","def relu(x):\n","    return jnp.maximum(0, x)\n","\n","def sigmoid(x):\n","    return 1 / (1 + jnp.exp(-x))\n","\n","def tanh(x):\n","    return jnp.tanh(x)\n","\n","def softmax(x):\n","    exps = jnp.exp(x - jnp.max(x))\n","    return exps / exps.sum(axis=-1, keepdims=True)\n","\n","# Define the neural network forward pass with activation function indices\n","def forward_pass(params, x, activation_indices):\n","    activations_output = x\n","    for i, (w, b) in enumerate(params[:-1]):\n","        outputs = jnp.dot(activations_output, w) + b\n","        activations_output = apply_activation(outputs, activation_indices[i])\n","    final_w, final_b = params[-1]\n","    return apply_activation(jnp.dot(activations_output, final_w) + final_b,activation_indices[-1])\n","    # return jnp.dot(activations_output, final_w) + final_b\n","\n","# Helper function to apply activation functions using lax.cond\n","def apply_activation(x, activation_index):\n","    return lax.switch(activation_index, [relu, sigmoid, tanh, softmax], x)\n","\n","# Loss function: Mean Squared Error\n","def process_tokens_single_example(params1, params2, params3, x, activation_indices1, activation_indices2, activation_indices3):\n","  for layer_idx in range(int(np.log2(x.shape[0]))):\n","    if layer_idx == 0:\n","      temp = forward_pass(params1, x, activation_indices1)\n","    else:\n","      temp = forward_pass(params1, input_next_layer, activation_indices1)\n","    input_next_layer = jnp.zeros(shape = (temp.shape[0] // 2 , 2*768))\n","    for k in range(temp.shape[0]):\n","      if k % 2 == 0:\n","          input_next_layer = input_next_layer.at[k // 2,:768].set(temp[k])\n","      else:\n","        input_next_layer = input_next_layer.at[k // 2 ,768:].set(temp[k])\n","  EO = forward_pass(params1, input_next_layer, activation_indices1)\n","  return forward_pass(params2, EO, activation_indices2)\n","def loss_single_example(params1, params2, params3, x, y, activation_indices1, activation_indices2, activation_indices3):\n","    next_predicted_token_embedding = process_tokens_single_example(params1, params2, params3, x, activation_indices1, activation_indices2, activation_indices3)\n","    return jnp.sum(jnp.abs(next_predicted_token_embedding - y))\n","loss_vectorized = jax.vmap(fun = loss_single_example, in_axes=(None,None,None,0,0,None,None,None))\n","process_tokens_vectorized = jax.vmap(fun = process_tokens_single_example, in_axes=(None, None, None, 0, None, None, None))\n","def learning_rate_decay(initial_lr, epoch, decay_rate=0.1, decay_steps=10):\n","    return initial_lr * (decay_rate ** (epoch // decay_steps))\n","\n","def loss(params1, params2, params3, x, y, activation_indices1, activation_indices2, activation_indices3):\n","    return jnp.sum(loss_vectorized(params1, params2, params3, x, y, activation_indices1, activation_indices2, activation_indices3))\n","# Training step\n","@jit\n","def update(params1, params2, params3, opt_state1, opt_state2, opt_state3, x, y, activation_indices1, activation_indices2, activation_indices3, t, lr):\n","  batch_size = 1024\n","  for idx in range(0, y.shape[0] - y.shape[0] % batch_size, batch_size):\n","    x_batch = x[idx:idx + batch_size]\n","    y_batch = y[idx:idx + batch_size]\n","    grads = grad(loss, argnums=(0,1,2))(params1, params2, params3, x_batch, y_batch, activation_indices1, activation_indices2, activation_indices3)\n","    params1, opt_state1 = adam_update(params1, grads[0], opt_state1, t, lr)\n","    params2, opt_state2 = adam_update(params2, grads[1], opt_state2, t, lr)\n","    params3, opt_state3 = adam_update(params3, grads[2], opt_state3, t, lr)\n","  return params1, params2, params3, opt_state1, opt_state2, opt_state3\n","\n","\n","# Example usage\n","layer_sizes1 = [2*768,  768]  # Depth and width of the first network\n","layer_sizes2 = [768,  768]   # Depth and width of the second network\n","layer_sizes3 = [768,  768]   # Depth and width of the thrid network\n","activation_indices1 = [2]  # Activation functions for each layer of the first network: 0=ReLU, 1=Sigmoid, 2=Tanh, 3=Softmax\n","activation_indices2 = [2]  # Activation functions for each layer of the second network\n","activation_indices3 = [2]  # Activation functions for each layer of the thrid network\n","\n","key = random.PRNGKey(0)\n","params1 = init_network_params(layer_sizes1, key)\n","key = random.PRNGKey(0)\n","params2 = init_network_params(layer_sizes2, key)\n","key = random.PRNGKey(0)\n","params3 = init_network_params(layer_sizes3, key)\n","\n","opt_state1 = init_adam_state(params1)\n","opt_state2 = init_adam_state(params2)\n","opt_state3 = init_adam_state(params3)\n","\n","store = loss(params1, params2, params3, x, y, activation_indices1, activation_indices2, activation_indices3)/(x.shape[0]*768)\n","print(jnp.mean(jnp.abs(y)))\n","store = loss(params1, params2, params3, x, y, activation_indices1, activation_indices2, activation_indices3)/(x.shape[0]*768)\n","print(jnp.mean(jnp.abs(y)))\n","print(store)\n","# Training loop"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NTfbsJ_hVKgG"},"outputs":[],"source":["import pickle\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense , LayerNormalization, Dropout\n","from tensorflow.keras.utils import to_categorical\n","from keras.models import load_model\n","with open('params1_64', 'rb') as f:\n","    params1 = pickle.load(f)\n","with open('params2_64', 'rb') as f:\n","    params2 = pickle.load(f)\n","perdector = load_model('perdector_64.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XLg2jWMaWSkI"},"outputs":[],"source":["print(loss(params1, params2,params3, x, y, activation_indices1, activation_indices2,activation_indices3)/(x.shape[0]*768))\n","print(loss(params1, params2,params3, x_test, y_test, activation_indices1, activation_indices2,activation_indices3)/(x_test.shape[0]*768))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S_wzfNojWobS"},"outputs":[],"source":["perdector.compile(optimizer='adam',\n","              loss='sparse_categorical_crossentropy',  # Use categorical crossentropy for multi-class classification\n","              metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vmxz04XqWpr1"},"outputs":[],"source":["LLM_x_test = process_tokens_vectorized(params1, params2,params3, x_test, activation_indices1, activation_indices2,activation_indices3)\n","LLM_x_test = np.array(jnp.transpose(LLM_x_test , (1,0,2))[0])\n","print(LLM_x_test.shape)\n","LLM_y_test = np.array(LLM_y_test)\n","LLM_y_test =  LLM_y_test.reshape(-1)\n","print(type(LLM_x_test),type(LLM_y_test))\n","print(LLM_x_test.shape,LLM_y_test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kgUOMo-sWr9C"},"outputs":[],"source":["# process_tokens_vectorized = jax.vmap(fun = process_tokens_single_example, in_axes=(None, None,  0, None, None))\n","LLM_x = process_tokens_vectorized(params1, params2,params3, x, activation_indices1, activation_indices2,activation_indices3)\n","LLM_x = np.array(jnp.transpose(LLM_x , (1,0,2))[0])\n","print(LLM_x.shape)\n","LLM_y = np.array(LLM_y)\n","LLM_y =  LLM_y.reshape(-1)\n","print(type(LLM_x),type(LLM_y))\n","print(LLM_x.shape,LLM_y.shape)\n","perdector.evaluate(LLM_x_test,LLM_y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YJ3oVTSWfHen"},"outputs":[],"source":["output = perdector.predict(LLM_x_test)\n","print(vocab)\n","vocab_list = []\n","for word in vocab:\n","  vocab_list.append(word)"]},{"cell_type":"code","source":["print(np.argmax(output, axis=1))"],"metadata":{"id":"5liU3Mhd8eZD"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KYGeTPSKiSbO"},"outputs":[],"source":["predicted_classes = np.argmax(output, axis=1)\n","answer_in_word = []\n","for i in predicted_classes:\n","  answer_in_word.append(vocab_list[i])  w\n","print(answer_in_word[:80])"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}