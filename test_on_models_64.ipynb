{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPzBnhNa37W0bNgSdLv12Z/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fiDX9dzIU8wv","executionInfo":{"status":"ok","timestamp":1723547922470,"user_tz":-330,"elapsed":11816,"user":{"displayName":"R five","userId":"13420369184929705110"}},"outputId":"51d8a7e7-2b6e-46c5-c536-6adce910135c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.20.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n","Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.7.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n","Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n","Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n","Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n","Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.7.1)\n","Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n","Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.7.4)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.6)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.3)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.16.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"]}],"source":["!pip install datasets\n","!pip install --upgrade tensorflow\n","# !pip install --upgrade jax"]},{"cell_type":"code","source":["import jax\n","import jax.numpy as jnp\n","from jax import grad, jit, random\n","from jax import lax\n","from jax.tree_util import tree_map\n","import numpy as np\n","from jax import grad, jit, random\n","from jax import lax\n","from jax.tree_util import tree_map\n","from transformers import GPT2Model, GPT2Tokenizer\n","import torch\n","import numpy as np\n","from datasets import load_dataset"],"metadata":{"id":"Co8cfUW4VEb1","executionInfo":{"status":"ok","timestamp":1723547930560,"user_tz":-330,"elapsed":8097,"user":{"displayName":"R five","userId":"13420369184929705110"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Load the GPT-2 tokenizer and model\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","model_gpt = GPT2Model.from_pretrained('gpt2')\n","vocab = tokenizer.get_vocab()\n","model_gpt.eval()\n","\n","def break_word_gpt(text):\n","    tokens = tokenizer.tokenize(text)\n","    return tokens\n","\n","def break_into_id(text):\n","    return tokenizer.encode(text, add_special_tokens=False)\n","\n","def encode_word_gpt(word):\n","    token_ids = tokenizer.encode(word, add_special_tokens=False)\n","    token_tensor = torch.tensor([token_ids])\n","    with torch.no_grad():\n","        token_embeddings = model_gpt.wte(token_tensor)\n","    return token_embeddings\n","\n","# Load the PTB text dataset\n","dataset = load_dataset('ptb_text_only')\n","train_data = dataset['train']\n","test_data = dataset['test']\n","print(dataset)\n","\n","def read_batch_file(start_index, size, data):\n","    data_token = []\n","    for sentence_pair_index in range(start_index, start_index + size):\n","        if sentence_pair_index < len(data):\n","            data_token.append(break_word_gpt(data[sentence_pair_index][\"sentence\"]))\n","    return data_token\n","def make_data(no_token,data):\n","  data_tokens = read_batch_file(0, int(len(train_data)/1), train_data)\n","  encode_tokens , encode_tokens_test ,x , y, y_LLM = [],[],[],[],[]\n","  temp = np.zeros(2*768)\n","  for i in data_tokens:\n","      encode_tokens.append(encode_word_gpt(i)[0])\n","  for i in range(int(len(data)/1)):\n","      if len(data_tokens[i]) > no_token:\n","          y.append(encode_tokens[i][no_token])\n","          y_LLM.append(break_into_id([data_tokens[i][no_token]]))\n","  x = np.zeros(shape=(len(y),no_token // 2,2*768))\n","  count_index = 0\n","  for i in range(int(len(data)/1)):\n","      if len(data_tokens[i]) > no_token:\n","          for k in range(no_token):\n","              if k % 2 == 0:\n","                  x[count_index][int(k/2)][:768] = encode_tokens[i][k]\n","              else:\n","                  x[count_index][int(k/2)][768:] = encode_tokens[i][k]\n","          count_index += 1\n","  x = np.transpose(np.transpose(x))\n","  y = jnp.array(y)\n","  x = jnp.array(x)\n","  return (x, y, y_LLM)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1vh1agGjVGCJ","executionInfo":{"status":"ok","timestamp":1723547939103,"user_tz":-330,"elapsed":8559,"user":{"displayName":"R five","userId":"13420369184929705110"}},"outputId":"b35d5e6e-4611-4dbb-c046-abfe36b49876"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['sentence'],\n","        num_rows: 42068\n","    })\n","    test: Dataset({\n","        features: ['sentence'],\n","        num_rows: 3761\n","    })\n","    validation: Dataset({\n","        features: ['sentence'],\n","        num_rows: 3370\n","    })\n","})\n"]}]},{"cell_type":"code","source":["x, y, LLM_y = make_data(64, train_data)\n","x_test, y_test, LLM_y_test = make_data(64, test_data)\n","print(x.shape, y.shape)\n","print(x_test.shape, y_test.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XtKUAYECVHxw","executionInfo":{"status":"ok","timestamp":1723547984923,"user_tz":-330,"elapsed":45834,"user":{"displayName":"R five","userId":"13420369184929705110"}},"outputId":"a91018b7-57e4-4f34-f6fd-347b23ecc46b"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["(311, 32, 1536) (311, 768)\n","(33, 32, 1536) (33, 768)\n"]}]},{"cell_type":"code","source":["# Initialize network parameters\n","def init_network_params(layer_sizes, key):\n","    keys = random.split(key, len(layer_sizes))\n","    return [(random.normal(k, (m, n)) * 0.01, random.normal(k, (n,)) * 0.01)\n","            for m, n, k in zip(layer_sizes[:-1], layer_sizes[1:], keys)]\n","\n","# Initialize Adam optimizer state\n","def init_adam_state(params):\n","    return [(jnp.zeros_like(w), jnp.zeros_like(w), jnp.zeros_like(b), jnp.zeros_like(b)) for w, b in params]\n","\n","# Adam optimizer update\n","def adam_update(params, grads, opt_state, t, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8):\n","    new_params = []\n","    new_opt_state = []\n","\n","    for (w, b), (dw, db), (m_w, v_w, m_b, v_b) in zip(params, grads, opt_state):\n","        m_w = beta1 * m_w + (1 - beta1) * dw\n","        v_w = beta2 * v_w + (1 - beta2) * (dw ** 2)\n","        m_b = beta1 * m_b + (1 - beta1) * db\n","        v_b = beta2 * v_b + (1 - beta2) * (db ** 2)\n","\n","        m_w_hat = m_w / (1 - beta1 ** t)\n","        v_w_hat = v_w / (1 - beta2 ** t)\n","        m_b_hat = m_b / (1 - beta1 ** t)\n","        v_b_hat = v_b / (1 - beta2 ** t)\n","\n","        w = w - lr * m_w_hat / (jnp.sqrt(v_w_hat) + eps)\n","        b = b - lr * m_b_hat / (jnp.sqrt(v_b_hat) + eps)\n","\n","        new_params.append((w, b))\n","        new_opt_state.append((m_w, v_w, m_b, v_b))\n","\n","    return new_params, new_opt_state\n","\n","# Activation functions\n","def relu(x):\n","    return jnp.maximum(0, x)\n","\n","def sigmoid(x):\n","    return 1 / (1 + jnp.exp(-x))\n","\n","def tanh(x):\n","    return jnp.tanh(x)\n","\n","def softmax(x):\n","    exps = jnp.exp(x - jnp.max(x))\n","    return exps / exps.sum(axis=-1, keepdims=True)\n","\n","# Define the neural network forward pass with activation function indices\n","def forward_pass(params, x, activation_indices):\n","    activations_output = x\n","    for i, (w, b) in enumerate(params[:-1]):\n","        outputs = jnp.dot(activations_output, w) + b\n","        activations_output = apply_activation(outputs, activation_indices[i])\n","    final_w, final_b = params[-1]\n","    return apply_activation(jnp.dot(activations_output, final_w) + final_b,activation_indices[-1])\n","    # return jnp.dot(activations_output, final_w) + final_b\n","\n","# Helper function to apply activation functions using lax.cond\n","def apply_activation(x, activation_index):\n","    return lax.switch(activation_index, [relu, sigmoid, tanh, softmax], x)\n","\n","# Loss function: Mean Squared Error\n","def process_tokens_single_example(params1, params2, x, activation_indices1, activation_indices2):\n","  for layer_idx in range(int(np.log2(x.shape[0]))):\n","    if layer_idx == 0:\n","      temp = forward_pass(params1, x, activation_indices1)\n","    else:\n","      temp = forward_pass(params1, input_next_layer, activation_indices1)\n","    input_next_layer = jnp.zeros(shape = (temp.shape[0] // 2 , 2*768))\n","    for k in range(temp.shape[0]):\n","      if k % 2 == 0:\n","          input_next_layer = input_next_layer.at[k // 2,:768].set(temp[k])\n","      else:\n","        input_next_layer = input_next_layer.at[k // 2 ,768:].set(temp[k])\n","  EO = forward_pass(params1, input_next_layer, activation_indices2)\n","  return forward_pass(params2, EO, activation_indices2)\n","def loss_single_example(params1, params2, x, y, activation_indices1, activation_indices2):\n","    next_predicted_token_embedding = process_tokens_single_example(params1, params2, x, activation_indices1, activation_indices2)\n","    return jnp.sum(jnp.abs(next_predicted_token_embedding - y))\n","\n","loss_vectorized = jax.vmap(fun = loss_single_example, in_axes=(None,None,0,0,None,None))\n","process_tokens_vectorized = jax.vmap(fun = process_tokens_single_example, in_axes=(None, None, 0, None, None))\n","\n","\n","def loss(params1, params2,  x, y, activation_indices1, activation_indices2):\n","    return jnp.sum(loss_vectorized(params1, params2, x, y, activation_indices1, activation_indices2))\n","\n","# Example usage\n","layer_sizes1 = [2*768,  768]  # Depth and width of the first network\n","layer_sizes2 = [768,  768]   # Depth and width of the second network\n","\n","activation_indices1 = [2]  # Activation functions for each layer of the first network: 0=ReLU, 1=Sigmoid, 2=Tanh, 3=Softmax\n","activation_indices2 = [2]  # Activation functions for each layer of the second network"],"metadata":{"id":"S8pFDt1AVIyq","executionInfo":{"status":"ok","timestamp":1723547984923,"user_tz":-330,"elapsed":12,"user":{"displayName":"R five","userId":"13420369184929705110"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["import pickle\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense , LayerNormalization, Dropout\n","from tensorflow.keras.utils import to_categorical\n","from keras.models import load_model\n","with open('params1_64_tokens', 'rb') as f:\n","    params1 = pickle.load(f)\n","with open('params2_64_tokens', 'rb') as f:\n","    params2 = pickle.load(f)\n","perdector = load_model('perdector_64.h5')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NTfbsJ_hVKgG","executionInfo":{"status":"ok","timestamp":1723547989494,"user_tz":-330,"elapsed":4582,"user":{"displayName":"R five","userId":"13420369184929705110"}},"outputId":"87d268f8-c85a-4620-f3a0-804d5217003f"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]}]},{"cell_type":"code","source":["print(loss(params1, params2, x, y, activation_indices1, activation_indices2)/(x.shape[0]*768))\n","print(loss(params1, params2, x_test, y_test, activation_indices1, activation_indices2)/(x_test.shape[0]*768))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XLg2jWMaWSkI","executionInfo":{"status":"ok","timestamp":1723547998063,"user_tz":-330,"elapsed":8582,"user":{"displayName":"R five","userId":"13420369184929705110"}},"outputId":"d6f2d44d-9f28-4425-92dd-63b59ada83ba"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["0.03733433\n","0.027458366\n"]}]},{"cell_type":"code","source":["perdector.compile(optimizer='adam',\n","              loss='sparse_categorical_crossentropy',  # Use categorical crossentropy for multi-class classification\n","              metrics=['accuracy'])"],"metadata":{"id":"S_wzfNojWobS","executionInfo":{"status":"ok","timestamp":1723547998063,"user_tz":-330,"elapsed":14,"user":{"displayName":"R five","userId":"13420369184929705110"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["LLM_x_test = process_tokens_vectorized(params1, params2, x_test, activation_indices1, activation_indices2)\n","LLM_x_test = np.array(jnp.transpose(LLM_x_test , (1,0,2))[0])\n","print(LLM_x_test.shape)\n","LLM_y_test = np.array(LLM_y_test)\n","LLM_y_test =  LLM_y_test.reshape(-1)\n","print(type(LLM_x_test),type(LLM_y_test))\n","print(LLM_x_test.shape,LLM_y_test.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vmxz04XqWpr1","executionInfo":{"status":"ok","timestamp":1723547998064,"user_tz":-330,"elapsed":14,"user":{"displayName":"R five","userId":"13420369184929705110"}},"outputId":"bf95a2db-23f5-4246-b4cf-fa784a3f72b8"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["(33, 768)\n","<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n","(33, 768) (33,)\n"]}]},{"cell_type":"code","source":["process_tokens_vectorized = jax.vmap(fun = process_tokens_single_example, in_axes=(None, None,  0, None, None))\n","LLM_x = process_tokens_vectorized(params1, params2, x, activation_indices1, activation_indices2)\n","LLM_x = np.array(jnp.transpose(LLM_x , (1,0,2))[0])\n","print(LLM_x.shape)\n","LLM_y = np.array(LLM_y)\n","LLM_y =  LLM_y.reshape(-1)\n","print(type(LLM_x),type(LLM_y))\n","print(LLM_x.shape,LLM_y.shape)\n","perdector.evaluate(LLM_x_test,LLM_y_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kgUOMo-sWr9C","executionInfo":{"status":"ok","timestamp":1723548001747,"user_tz":-330,"elapsed":3695,"user":{"displayName":"R five","userId":"13420369184929705110"}},"outputId":"739b39ba-51d3-43f4-b714-b720aab0180d"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["(311, 768)\n","<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n","(311, 768) (311,)\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 779ms/step - accuracy: 1.0000 - loss: 0.0349\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.03491758555173874, 1.0]"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["perdector.evaluate(LLM_x,LLM_y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KMv1eJGlZCIG","executionInfo":{"status":"ok","timestamp":1723548002846,"user_tz":-330,"elapsed":1105,"user":{"displayName":"R five","userId":"13420369184929705110"}},"outputId":"806b836e-e735-451b-f403-c50d82edc60c"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - accuracy: 1.0000 - loss: 0.0445\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.052489861845970154, 1.0]"]},"metadata":{},"execution_count":11}]}]}